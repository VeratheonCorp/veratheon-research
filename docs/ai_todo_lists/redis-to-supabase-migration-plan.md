# Redis to Supabase Migration Plan - Execution Guide

## Current Redis Usage Summary

### 1. Job Tracking (`src/lib/job_tracker.py`)
- Stores job metadata (UUID, type, symbol, status, timestamps, steps, results, errors)
- Redis keys: `job:{job_id}`, `job_by_symbol:{symbol}` (24h TTL)
- Job statuses: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED

### 2. Cache System (`src/lib/redis_cache.py`)
- Caches 13 report types: historical_earnings, financial_statements, management_guidance, forward_pe, news_sentiment, trade_ideas, cross_reference, comprehensive_report, key_insights, company_overview, global_quote, earnings_projections, peer_group
- Redis keys: `report:{type}:{symbol}:{YYYYMMDD}`, `analysis:{type}:{symbol}:{YYYYMMDD}` (24h/1h TTL)
- Used by 13 `*_reporting_task.py` files

### 3. Real-time Status Updates (`src/tasks/common/status_update_task.py`)
- **NOT ACTUALLY USED** - Code exists but is never imported/called
- Frontend uses simple HTTP polling every 3 seconds instead
- Can be deleted during migration

## Supabase Schema to Create

### Table 1: `research_jobs` (✓ Created)

```sql
create table public.research_jobs (
  id bigint generated by default as identity not null,
  created_at timestamp with time zone not null default now(),
  updated_at timestamp without time zone not null default now(),
  symbol text not null,
  status text not null,
  completed_at timestamp without time zone null,
  failed_at timestamp with time zone null,
  metadata jsonb null,
  error text null,
  constraint research_jobs_pkey primary key (id)
) tablespace pg_default;
```

**Notes:**
- Uses `bigint` id instead of UUID for simpler lookups
- Removed `job_type` (always 'research')
- Removed `steps` and `result` (not needed - data stored elsewhere)
- Status values: 'pending', 'running', 'completed', 'failed', 'cancelled'
- No additional indexes needed (primary key sufficient)

### Table 2: `research_cache` (✓ Created)

```sql
create table public.research_cache (
  id bigint generated by default as identity not null,
  created_at timestamp with time zone not null default now(),
  expires_at timestamp without time zone null,
  symbol text not null,
  cache_key text null,
  cache_date date null,
  report_type text not null,
  cache_type text not null,
  data jsonb not null,
  metadata jsonb null,
  constraint research_cache_pkey primary key (id),
  constraint research_cache_cache_key_key unique (cache_key)
) tablespace pg_default;
```

**Notes:**
- Cache types: 'report', 'analysis'
- `expires_at` serves as **insurance** if nightly batch archival job fails
- `cache_key` format: `{cache_type}:{report_type}:{symbol}:{YYYYMMDD}[:kwargs_hash]`
- **Nightly batch job** moves expired entries to `research_archive` table
- No additional indexes needed for now

### Table 3: `research_docs` (✓ Already Exists)

```sql
create table public.research_docs (
  id bigint generated by default as identity not null,
  created_at timestamp with time zone not null default now(),
  updated_at timestamp without time zone not null default now(),
  embedding extensions.vector null,
  metadata jsonb null,
  content text null,
  title text null,
  token_count bigint null,
  constraint research_docs_pkey primary key (id)
) tablespace pg_default;
```

**Notes:**
- Already created for RAG embeddings
- Stores comprehensive research reports with vector embeddings
- No changes needed

### Table 4: `user_research_history` (✓ Created)

```sql
create table public.user_research_history (
  id bigint generated by default as identity not null,
  created_at timestamp with time zone not null default now(),
  updated_at timestamp without time zone not null default now(),
  user_id text not null default 'anonymous'::text,
  symbol text not null,
  job_id bigint null,
  metadata jsonb null,
  constraint user_research_history_pkey primary key (id)
) tablespace pg_default;
```

**Purpose:**
- Track which symbols each user has researched
- Link user activity to research jobs (`job_id`)
- `user_id` defaults to 'anonymous' (auth can be added later)
- `metadata` JSONB for flexible data storage:
  - `is_favorite` - User bookmarked symbol
  - `view_count` - Number of times viewed
  - `last_viewed_at` - Last view timestamp
  - `research_completed` - Whether job finished
  - Custom user preferences, alerts, notes, etc.

**Notes:**
- Simplified design - no foreign key constraint for flexibility
- No additional indexes for now - can add later based on query patterns
- Metadata allows schema evolution without migrations

### Table 5: `system_logs` (✓ Created)

```sql
create table public.system_logs (
  id bigint generated by default as identity not null,
  created_at timestamp with time zone not null default now(),
  log_level text not null,
  component text not null,
  message text not null,
  job_id text null,
  symbol text null,
  stack_trace text null,
  metadata jsonb null,
  constraint system_logs_pkey primary key (id)
) tablespace pg_default;
```

**Purpose:**
- Centralized error tracking and debugging for solo developer
- Easy to query recent errors/warnings from one place
- Link logs to specific jobs and symbols for debugging
- `log_level`: 'error', 'warning', 'info', 'debug'
- `component`: Which part of system (e.g., 'job_tracker', 'supabase_cache', 'historical_earnings_flow')
- `job_id`: text field (flexible for bigint or UUID)
- `stack_trace`: Full Python traceback for errors
- `metadata`: Additional context (request params, state, etc.)

**Notes:**
- No indexes initially - can add based on query patterns
- `job_id` as text for flexibility (supports bigint, UUID, or other formats)

**Usage Example:**
```python
# In exception handlers
supabase.table('system_logs').insert({
    'log_level': 'error',
    'component': 'supabase_cache',
    'message': f'Failed to cache report for {symbol}',
    'symbol': symbol,
    'job_id': str(job_id),
    'stack_trace': traceback.format_exc(),
    'metadata': {'report_type': report_type, 'ttl': ttl}
}).execute()
```

## Migration Execution Plan

### STEP 1: Database Schema Setup

**Actions:**
1. ✓ Created `research_jobs` table manually
2. ✓ Created `research_cache` table manually
3. ✓ Verified `research_docs` table exists
4. ✓ Created `user_research_history` table manually
5. ✓ Created `system_logs` table manually
6. **Enable Supabase Realtime** on `research_jobs` table for live updates
7. ✓ Set up environment variables in `.env`

**Status:** All tables created ✓ | Realtime pending

**Enable Realtime (run in Supabase SQL editor or psql):**
```sql
-- Enable Realtime for research_jobs table
alter publication supabase_realtime add table research_jobs;
```

### STEP 2: Create Supabase Client & Utilities

**Actions:**
1. Create `src/lib/supabase_client.py` - Initialize Supabase client with connection pooling
2. Create `src/lib/supabase_job_tracker.py` - Complete replacement for `src/lib/job_tracker.py`
3. Create `src/lib/supabase_cache.py` - Complete replacement for `src/lib/redis_cache.py`
4. Create `src/lib/supabase_rag.py` - RAG operations for embeddings + research_docs integration
5. Create `src/lib/supabase_logger.py` - Centralized logging to system_logs table

**Files to create:**
- `src/lib/supabase_client.py`
- `src/lib/supabase_job_tracker.py`
- `src/lib/supabase_cache.py`
- `src/lib/supabase_rag.py`
- `src/lib/supabase_logger.py`

**Interface Requirements:**

`supabase_job_tracker.py` must implement:
- `create_job(job_type, symbol, metadata) -> str` (returns job_id)
- `update_job_status(job_id, status, step, result, error) -> bool`
- `get_job_status(job_id) -> Optional[Dict]`
- `get_job_by_symbol(symbol) -> Optional[str]`
- `cancel_job(job_id) -> bool`
- `list_jobs(limit) -> List[Dict]`
- `get_job_tracker()` singleton

`supabase_cache.py` must implement:
- `get_cached_report(report_type, symbol, **kwargs) -> Optional[Dict]`
- `cache_report(report_type, symbol, data, ttl, **kwargs) -> bool`
- `get_cached_analysis(analysis_type, symbol, **kwargs) -> Optional[Dict]`
- `cache_analysis(analysis_type, symbol, data, ttl, **kwargs) -> bool`
- `invalidate_cache(pattern) -> int`
- `get_cache_info(symbol) -> Dict`
- `get_redis_cache()` singleton (renamed but same interface)

`supabase_rag.py` must implement:
- `add_document(content, title, metadata, symbol, report_type) -> bool` (generates embedding)
- `search_documents(query, limit, symbol_filter) -> List[Dict]`
- `delete_old_documents(days) -> int`

### STEP 3: Update All Imports

**Actions:**
1. Replace `from src.lib.redis_cache import get_redis_cache` with `from src.lib.supabase_cache import get_supabase_cache as get_redis_cache` in all 13 `*_reporting_task.py` files
2. Replace `from src.lib.job_tracker import get_job_tracker, JobStatus` with `from src.lib.supabase_job_tracker import get_job_tracker, JobStatus` in `server/api.py`
3. Delete `src/tasks/common/status_update_task.py` (unused code)

**Files to modify:**
- `src/tasks/historical_earnings/historical_earnings_reporting_task.py`
- `src/tasks/financial_statements/financial_statements_reporting_task.py`
- `src/tasks/management_guidance/management_guidance_reporting_task.py`
- `src/tasks/forward_pe/forward_pe_reporting_task.py`
- `src/tasks/news_sentiment/news_sentiment_reporting_task.py`
- `src/tasks/trade_ideas/trade_ideas_reporting_task.py`
- `src/tasks/cross_reference/cross_reference_reporting_task.py`
- `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py`
- `src/tasks/comprehensive_report/key_insights_reporting_task.py`
- `src/tasks/company_overview/company_overview_reporting_task.py`
- `src/tasks/global_quote/global_quote_reporting_task.py`
- `src/tasks/earnings_projections/earnings_projections_reporting_task.py`
- `src/tasks/common/peer_group_reporting_task.py`
- `server/api.py`

**Files to delete:**
- `src/tasks/common/status_update_task.py` (unused code)

### STEP 4: Add RAG Integration for Comprehensive Reports

**Actions:**
1. Update `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py` to also call `supabase_rag.add_document()` after caching
2. Generate embedding and store in `research_docs` table with metadata

**Files to modify:**
- `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py`

### STEP 5: Update Environment Variables

**Actions:**
1. Update `.env.example` to include Supabase variables
2. Document the removal of `REDIS_URL`

**Files to modify:**
- `.env.example`

**Required environment variables:**
```bash
SUPABASE_URL=http://localhost:54321
SUPABASE_KEY=your-anon-key-here
SUPABASE_SERVICE_KEY=your-service-role-key-here  # For backend operations
```

### STEP 6: Remove Redis Dependencies

**Actions:**
1. Delete `src/lib/redis_cache.py`
2. Delete `src/lib/job_tracker.py`
3. Remove Redis service from `docker-compose.yml`
4. Remove `redis` from `pyproject.toml` dependencies
5. Run `uv sync` to update lockfile

**Files to delete:**
- `src/lib/redis_cache.py`
- `src/lib/job_tracker.py`
- `src/tasks/common/status_update_task.py` (unused code - never imported)

**Files to modify:**
- `docker-compose.yml`
- `pyproject.toml`

### STEP 7: Update Cache Retrieval Tasks

**Actions:**
Update all cache retrieval tasks to use Supabase cache instead of Redis:
- `src/tasks/cache_retrieval/historical_earnings_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/financial_statements_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/management_guidance_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/forward_pe_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/news_sentiment_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/trade_ideas_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/cross_reference_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/comprehensive_report_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/key_insights_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/company_overview_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/global_quote_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/earnings_projections_cache_retrieval_task.py`


### STEP 8: Removed
# Not needed

**Note:** Using `expires_at` for cleanup - no archive table needed

### STEP 9: Update Tests

**Actions:**
1. Update `tests/conftest.py` to set up Supabase test fixtures
2. Update `tests/unit/tasks/test_common_tasks.py` to use Supabase mocks
3. Update any test files that mock Redis to mock Supabase instead

**Files to modify:**
- `tests/conftest.py`
- `tests/unit/tasks/test_common_tasks.py`
- Any other test files using Redis

### STEP 10: Update Documentation

**Actions:**
1. Update `CLAUDE.md` to remove Redis references and add Supabase setup instructions
2. Update `README.md` (if exists) with new environment variables
3. Update `.windsurf/rules/architecture.md` to document Supabase usage

**Files to modify:**
- `CLAUDE.md`
- `.windsurf/rules/architecture.md`

### STEP 11: Update Frontend for Supabase Realtime

**Status:** REQUIRED - Replacing polling with real-time updates

**Actions:**
1. Add `@supabase/supabase-js` to `agent-ui/package.json`
2. Create Supabase client in frontend (`agent-ui/src/lib/supabase.ts`)
3. Update `agent-ui/src/routes/+page.svelte` to subscribe to `research_jobs` table changes
4. Remove polling interval (currently 3 seconds) in favor of real-time subscriptions
5. Listen for INSERT/UPDATE events on `research_jobs` where `job_id` matches current job

**Benefits:**
- Instant updates (no 3-second delay)
- Reduced backend load (no polling requests)
- Better UX with live status changes

## Implementation Notes

### Cache Key Generation
Maintain the same cache key format as Redis for compatibility:
- Reports: `report:{report_type}:{symbol}:{YYYYMMDD}[:kwargs_hash]`
- Analysis: `analysis:{analysis_type}:{symbol}:{YYYYMMDD}[:kwargs_hash]`

### TTL Handling
- Reports: 24 hours (86400 seconds)
- Analysis: 1 hour (3600 seconds)
- Jobs: 7 days retention (configurable)

Calculate `expires_at = now() + interval '{ttl} seconds'` when inserting

### Cleanup Strategy
Run cleanup jobs via:
1. Background task in FastAPI server (startup event)
2. Cron job calling Python scripts
3. Manual execution as needed

### Testing Strategy
1. Run existing test suite to verify functionality
2. Test cache hit/miss scenarios
3. Test job lifecycle (create → running → completed/failed)
4. Test Realtime updates in UI
5. Verify embeddings are generated for comprehensive reports

## Success Criteria

- ✓ All 13 reporting tasks successfully cache to Supabase
- ✓ Job tracking fully functional via Supabase
- ✓ Job status polling working correctly (HTTP polling maintained)
- ✓ RAG embeddings generated for comprehensive reports
- ✓ All tests passing
- ✓ Redis completely removed from codebase
- ✓ Docker compose works without Redis service
- ✓ UI polling works correctly (3-second intervals maintained)

## Files Summary

**To Create (9 files):**
1. `src/lib/supabase_client.py`
2. `src/lib/supabase_job_tracker.py`
3. `src/lib/supabase_cache.py`
4. `src/lib/supabase_rag.py`
5. `src/lib/supabase_logger.py` (centralized error logging)
6. `src/jobs/cleanup_expired_cache.py` (daily)
7. `src/jobs/cleanup_old_jobs.py` (daily)
8. `src/jobs/cleanup_old_logs.py` (weekly - delete logs >30 days)
9. `agent-ui/src/lib/supabase.ts` (frontend Supabase client)

**SQL to run manually:**
1. ✓ Created all 5 tables
2. Enable Realtime: `alter publication supabase_realtime add table research_jobs;`

**To Delete (3 files):**
1. `src/lib/redis_cache.py`
2. `src/lib/job_tracker.py`
3. `src/tasks/common/status_update_task.py`

**To Modify (~32 files):**
- 13 reporting task files (import changes)
- 12 cache retrieval task files (import changes)
- `server/api.py` (import changes + track user research history)
- `agent-ui/src/routes/+page.svelte` (Realtime subscriptions)
- `agent-ui/package.json` (add @supabase/supabase-js)
- `docker-compose.yml` (remove Redis)
- `pyproject.toml` (remove redis dependency, add supabase)
- `.env.example` (add Supabase vars ✓)
- `CLAUDE.md` (update docs)
- `.windsurf/rules/architecture.md` (update architecture)
- `tests/conftest.py` (Supabase fixtures)
- `tests/unit/tasks/test_common_tasks.py` (remove status_update tests)

## Key Findings & Architecture Decisions

**Redis Pub/Sub is NOT actually used:**
- `src/tasks/common/status_update_task.py` exists but is never imported in source code (only in tests)
- Frontend uses simple HTTP polling every 3 seconds via `/api/research/status/${symbol}`
- No WebSocket, SSE, or Redis subscription on frontend
- This unused code will be deleted during migration

**Migrating to Supabase Realtime:**
1. Backend writes job status to Supabase `research_jobs` table
2. Frontend subscribes to Realtime updates on `research_jobs`
3. Instant updates when job status changes (no polling delay)
4. Cleaner architecture with built-in pub/sub

**User Tracking:**
- New `user_research_history` table tracks user research activity
- Links users to symbols and jobs
- Supports favorites, view counts, and analytics
- Defaults to `user_id = 'anonymous'` (auth can be added later)

**Cache Expiration:**
- Using `expires_at` column for cleanup (no archive table)
- Daily cleanup job deletes expired entries
- Simplified approach - archive not needed

## Execution Order

Execute steps in sequence 1→11. Each step should be completed and verified before moving to the next.
