# Redis to Supabase Migration Plan - Execution Guide

## Current Redis Usage Summary

### 1. Job Tracking (`src/lib/job_tracker.py`)
- Stores job metadata (UUID, type, symbol, status, timestamps, steps, results, errors)
- Redis keys: `job:{job_id}`, `job_by_symbol:{symbol}` (24h TTL)
- Job statuses: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED

### 2. Cache System (`src/lib/redis_cache.py`)
- Caches 13 report types: historical_earnings, financial_statements, management_guidance, forward_pe, news_sentiment, trade_ideas, cross_reference, comprehensive_report, key_insights, company_overview, global_quote, earnings_projections, peer_group
- Redis keys: `report:{type}:{symbol}:{YYYYMMDD}`, `analysis:{type}:{symbol}:{YYYYMMDD}` (24h/1h TTL)
- Used by 13 `*_reporting_task.py` files

### 3. Real-time Status Updates (`src/tasks/common/status_update_task.py`)
- **NOT ACTUALLY USED** - Code exists but is never imported/called
- Frontend uses simple HTTP polling every 3 seconds instead
- Can be deleted during migration

## Supabase Schema to Create

### Table 1: `research_jobs` (New)

```sql
create table public.research_jobs (
  id uuid primary key default gen_random_uuid(),
  job_type text not null,
  symbol text not null,
  status text not null check (status in ('pending', 'running', 'completed', 'failed', 'cancelled')),
  created_at timestamp with time zone not null default now(),
  updated_at timestamp with time zone not null default now(),
  completed_at timestamp with time zone null,
  failed_at timestamp with time zone null,
  metadata jsonb null,
  steps jsonb[] null default array[]::jsonb[],
  result jsonb null,
  error text null
);

create index idx_research_jobs_symbol_created on public.research_jobs(symbol, created_at desc);
create index idx_research_jobs_status on public.research_jobs(status);
```

### Table 2: `research_cache` (New)

```sql
create table public.research_cache (
  id bigint generated by default as identity not null,
  cache_type text not null check (cache_type in ('report', 'analysis')),
  report_type text not null,
  symbol text not null,
  cache_key text not null unique,
  cache_date date not null,
  created_at timestamp with time zone not null default now(),
  expires_at timestamp with time zone not null,
  data jsonb not null,
  metadata jsonb null,
  constraint research_cache_pkey primary key (id),
  constraint research_cache_key_unique unique (cache_key)
);

create index idx_research_cache_key on public.research_cache(cache_key);
create index idx_research_cache_symbol_date on public.research_cache(symbol, cache_date desc);
create index idx_research_cache_expires on public.research_cache(expires_at);
create index idx_research_cache_type_symbol on public.research_cache(report_type, symbol, cache_date desc);
```

### Table 3: `research_docs` (Already Exists)
✓ Already created for RAG embeddings - no changes needed

## Migration Execution Plan

### STEP 1: Database Schema Setup

**Actions:**
1. Create SQL migration file `supabase/migrations/001_redis_migration.sql` with both table definitions
2. Apply migration to local Supabase instance
3. Verify tables exist and indexes are created

**Files to create:**
- `supabase/migrations/001_redis_migration.sql`

### STEP 2: Create Supabase Client & Utilities

**Actions:**
1. Create `src/lib/supabase_client.py` - Initialize Supabase client with connection pooling
2. Create `src/lib/supabase_job_tracker.py` - Complete replacement for `src/lib/job_tracker.py`
3. Create `src/lib/supabase_cache.py` - Complete replacement for `src/lib/redis_cache.py`
4. Create `src/lib/supabase_rag.py` - RAG operations for embeddings + research_docs integration

**Files to create:**
- `src/lib/supabase_client.py`
- `src/lib/supabase_job_tracker.py`
- `src/lib/supabase_cache.py`
- `src/lib/supabase_rag.py`

**Interface Requirements:**

`supabase_job_tracker.py` must implement:
- `create_job(job_type, symbol, metadata) -> str` (returns job_id)
- `update_job_status(job_id, status, step, result, error) -> bool`
- `get_job_status(job_id) -> Optional[Dict]`
- `get_job_by_symbol(symbol) -> Optional[str]`
- `cancel_job(job_id) -> bool`
- `list_jobs(limit) -> List[Dict]`
- `get_job_tracker()` singleton

`supabase_cache.py` must implement:
- `get_cached_report(report_type, symbol, **kwargs) -> Optional[Dict]`
- `cache_report(report_type, symbol, data, ttl, **kwargs) -> bool`
- `get_cached_analysis(analysis_type, symbol, **kwargs) -> Optional[Dict]`
- `cache_analysis(analysis_type, symbol, data, ttl, **kwargs) -> bool`
- `invalidate_cache(pattern) -> int`
- `get_cache_info(symbol) -> Dict`
- `get_redis_cache()` singleton (renamed but same interface)

`supabase_rag.py` must implement:
- `add_document(content, title, metadata, symbol, report_type) -> bool` (generates embedding)
- `search_documents(query, limit, symbol_filter) -> List[Dict]`
- `delete_old_documents(days) -> int`

### STEP 3: Update All Imports

**Actions:**
1. Replace `from src.lib.redis_cache import get_redis_cache` with `from src.lib.supabase_cache import get_supabase_cache as get_redis_cache` in all 13 `*_reporting_task.py` files
2. Replace `from src.lib.job_tracker import get_job_tracker, JobStatus` with `from src.lib.supabase_job_tracker import get_job_tracker, JobStatus` in `server/api.py`
3. Delete `src/tasks/common/status_update_task.py` (unused code)

**Files to modify:**
- `src/tasks/historical_earnings/historical_earnings_reporting_task.py`
- `src/tasks/financial_statements/financial_statements_reporting_task.py`
- `src/tasks/management_guidance/management_guidance_reporting_task.py`
- `src/tasks/forward_pe/forward_pe_reporting_task.py`
- `src/tasks/news_sentiment/news_sentiment_reporting_task.py`
- `src/tasks/trade_ideas/trade_ideas_reporting_task.py`
- `src/tasks/cross_reference/cross_reference_reporting_task.py`
- `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py`
- `src/tasks/comprehensive_report/key_insights_reporting_task.py`
- `src/tasks/company_overview/company_overview_reporting_task.py`
- `src/tasks/global_quote/global_quote_reporting_task.py`
- `src/tasks/earnings_projections/earnings_projections_reporting_task.py`
- `src/tasks/common/peer_group_reporting_task.py`
- `server/api.py`

**Files to delete:**
- `src/tasks/common/status_update_task.py` (unused code)

### STEP 4: Add RAG Integration for Comprehensive Reports

**Actions:**
1. Update `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py` to also call `supabase_rag.add_document()` after caching
2. Generate embedding and store in `research_docs` table with metadata

**Files to modify:**
- `src/tasks/comprehensive_report/comprehensive_report_reporting_task.py`

### STEP 5: Update Environment Variables

**Actions:**
1. Update `.env.example` to include Supabase variables
2. Document the removal of `REDIS_URL`

**Files to modify:**
- `.env.example`

**Required environment variables:**
```bash
SUPABASE_URL=http://localhost:54321
SUPABASE_KEY=your-anon-key-here
SUPABASE_SERVICE_KEY=your-service-role-key-here  # For backend operations
```

### STEP 6: Remove Redis Dependencies

**Actions:**
1. Delete `src/lib/redis_cache.py`
2. Delete `src/lib/job_tracker.py`
3. Remove Redis service from `docker-compose.yml`
4. Remove `redis` from `pyproject.toml` dependencies
5. Run `uv sync` to update lockfile

**Files to delete:**
- `src/lib/redis_cache.py`
- `src/lib/job_tracker.py`
- `src/tasks/common/status_update_task.py` (unused code - never imported)

**Files to modify:**
- `docker-compose.yml`
- `pyproject.toml`

### STEP 7: Update Cache Retrieval Tasks

**Actions:**
Update all cache retrieval tasks to use Supabase cache instead of Redis:
- `src/tasks/cache_retrieval/historical_earnings_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/financial_statements_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/management_guidance_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/forward_pe_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/news_sentiment_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/trade_ideas_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/cross_reference_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/comprehensive_report_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/key_insights_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/company_overview_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/global_quote_cache_retrieval_task.py`
- `src/tasks/cache_retrieval/earnings_projections_cache_retrieval_task.py`

### STEP 8: Create Cleanup Jobs

**Actions:**
1. Create `src/jobs/cleanup_expired_cache.py` - Delete expired cache entries (runs daily)
2. Create `src/jobs/cleanup_old_jobs.py` - Delete jobs older than 7 days (runs daily)
3. Document how to run these as cron jobs or background tasks

**Files to create:**
- `src/jobs/cleanup_expired_cache.py`
- `src/jobs/cleanup_old_jobs.py`

### STEP 9: Update Tests

**Actions:**
1. Update `tests/conftest.py` to set up Supabase test fixtures
2. Update `tests/unit/tasks/test_common_tasks.py` to use Supabase mocks
3. Update any test files that mock Redis to mock Supabase instead

**Files to modify:**
- `tests/conftest.py`
- `tests/unit/tasks/test_common_tasks.py`
- Any other test files using Redis

### STEP 10: Update Documentation

**Actions:**
1. Update `CLAUDE.md` to remove Redis references and add Supabase setup instructions
2. Update `README.md` (if exists) with new environment variables
3. Update `.windsurf/rules/architecture.md` to document Supabase usage

**Files to modify:**
- `CLAUDE.md`
- `.windsurf/rules/architecture.md`

### STEP 11: (OPTIONAL) Add Supabase Realtime for Better UX

**Status:** OPTIONAL - Current polling works fine

**Actions (if implementing):**
1. Enable Realtime on `research_jobs` table
2. Update frontend to subscribe to job updates via Supabase Realtime
3. Remove polling interval in favor of real-time subscriptions
4. Add `@supabase/supabase-js` to `agent-ui/package.json`

**Note:** Current implementation uses HTTP polling every 3 seconds. This works fine, but Supabase Realtime would provide instant updates and reduce backend load.

## Implementation Notes

### Cache Key Generation
Maintain the same cache key format as Redis for compatibility:
- Reports: `report:{report_type}:{symbol}:{YYYYMMDD}[:kwargs_hash]`
- Analysis: `analysis:{analysis_type}:{symbol}:{YYYYMMDD}[:kwargs_hash]`

### TTL Handling
- Reports: 24 hours (86400 seconds)
- Analysis: 1 hour (3600 seconds)
- Jobs: 7 days retention (configurable)

Calculate `expires_at = now() + interval '{ttl} seconds'` when inserting

### Cleanup Strategy
Run cleanup jobs via:
1. Background task in FastAPI server (startup event)
2. Cron job calling Python scripts
3. Manual execution as needed

### Testing Strategy
1. Run existing test suite to verify functionality
2. Test cache hit/miss scenarios
3. Test job lifecycle (create → running → completed/failed)
4. Test Realtime updates in UI
5. Verify embeddings are generated for comprehensive reports

## Success Criteria

- ✓ All 13 reporting tasks successfully cache to Supabase
- ✓ Job tracking fully functional via Supabase
- ✓ Job status polling working correctly (HTTP polling maintained)
- ✓ RAG embeddings generated for comprehensive reports
- ✓ All tests passing
- ✓ Redis completely removed from codebase
- ✓ Docker compose works without Redis service
- ✓ UI polling works correctly (3-second intervals maintained)

## Files Summary

**To Create (7 files):**
1. `supabase/migrations/001_redis_migration.sql`
2. `src/lib/supabase_client.py`
3. `src/lib/supabase_job_tracker.py`
4. `src/lib/supabase_cache.py`
5. `src/lib/supabase_rag.py`
6. `src/jobs/cleanup_expired_cache.py`
7. `src/jobs/cleanup_old_jobs.py`

**To Delete (3 files):**
1. `src/lib/redis_cache.py`
2. `src/lib/job_tracker.py`
3. `src/tasks/common/status_update_task.py`

**To Modify (~30 files):**
- 13 reporting task files (import changes)
- 12 cache retrieval task files (import changes)
- `server/api.py` (import changes)
- `docker-compose.yml` (remove Redis)
- `pyproject.toml` (remove redis dependency, add supabase)
- `.env.example` (add Supabase vars)
- `CLAUDE.md` (update docs)
- `.windsurf/rules/architecture.md` (update architecture)
- `tests/conftest.py` (Supabase fixtures)
- `tests/unit/tasks/test_common_tasks.py` (remove status_update tests)

## Key Findings

**Redis Pub/Sub is NOT actually used:**
- `src/tasks/common/status_update_task.py` exists but is never imported in source code (only in tests)
- Frontend uses simple HTTP polling every 3 seconds via `/api/research/status/${symbol}`
- No WebSocket, SSE, or Redis subscription on frontend
- This unused code will be deleted during migration

**Current Status Update Flow:**
1. Backend writes job status to Redis (via `job_tracker.update_job_status()`)
2. Frontend polls HTTP endpoint every 3 seconds
3. Backend reads job status from Redis and returns to frontend
4. Simple but effective - no need for complex real-time infrastructure

## Execution Order

Execute steps in sequence 1→10. Step 11 (Realtime) is optional. Each step should be completed and verified before moving to the next.
